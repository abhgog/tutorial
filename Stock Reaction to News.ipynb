{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "It is known that stock prices and the health of the market are two very fickle variables - they change due to the slightest perturbations in world politics, economic downturns, and even results of sports championships. For example, after the 2016 presidential election, stocks fluctuated wildly, as the result of the election was extremely unexpected, which made investors anxious.\n",
    "\n",
    "![Viz](http://www.moneychoice.org/wp-content/uploads/2014/11/stocks.jpg)\n",
    "\n",
    "We wanted to explore the relationship between stock prices (we picked a couple of stocks below) and concurrent news articles published at the time - mainly we want to see if news can be a good predictor of whether a stock price will go up or down. We chose to go with this topic mainly because the influence of news and media has been under the magnifying glass for a long time, and we wanted to use key concepts that we used in our data science class to quantitatively assess whether or not the news has as much of an effect people say it does on the financial markets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline of Report - API's Used\n",
    "\n",
    "We will mainly focus on the stock prices of leading companies in various different sectors in order to gain a good variety (our methodologies could easily be applied to entire market indices, or the stock markets of entire nations). For news, we have decided to focus on four different news sources, three of which are financial, and one is not. \n",
    "\n",
    "The stocks we will be focusing on in this report are:\n",
    "\n",
    "- Google (GOOG)\n",
    "- Exxon-Mobil (XOM)\n",
    "- Citigroup (C)\n",
    "\n",
    "As you can see, these are all stocks from different sectors of the economy (tech, energy, and finance). To get data for the stocks, we used **Google Finance** to get data.\n",
    "\n",
    "The news sources whose tweets we will be pulling from are:\n",
    "\n",
    "- Wall Street Journal (@WSJ)\n",
    "- Bloomberg (@business)\n",
    "- Financial Times (@FinancialTimes)\n",
    "- New York Times (@nytimes)\n",
    "\n",
    "We chose a mix of financial and regular news sources, in order to get news of different varieties. In order to get data from these sources, as mentioned, we will be using the **Twitter API**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline of Report - Methodology\n",
    "\n",
    "Finally, how do these two different areas combine together? Good question. To figure out the correlation, and (hopefully), make a prediction on the stock prices, we need to do a bunch of things.\n",
    "\n",
    "1. Gather data and clean it up\n",
    "2. Visualize!\n",
    "3. Classify tweets as either having a positive effect or negative effect\n",
    "4. Use a learning method to predict stock prices\n",
    "\n",
    "A lot of these steps will be using methods we learned in class. As mentioned before, we will be using the Google Finance API and Twitter API to get financial and news data. \n",
    "\n",
    "Then, we will be using matplotlib to see if there is any high-level correlation between the stocks we selected and the news that we received through the tweets about those stocks in a given time frame.\n",
    "\n",
    "Finally, we will classify tweets as either \"good news\" or \"bad news\", using the NLTK packages, and their text classification capabilities. Using this, we will then compare different learning methods, such as support vector machines, random forest classifiers, and neural net, to figure out whether or not a stock price will increase or decrease based on given news articles. We will likely use data from the last few months for training, and data for this month as test data.\n",
    "\n",
    "Lets get started with the first part of the process, which is gathering the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import twitter #allows us to use the twitter api\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"svg\")\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Gathering of Data - Tweets\n",
    "\n",
    "We will be using the aforementioned news sources, as well as the Twitter API get the news. We'll start by initializing the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#API initialization (you need a consumer_key and secret - to read more about that look at the Twitter API documentation)\n",
    "with open(\"secret.json\", \"rt\") as fp:\n",
    "    params = json.load(fp)\n",
    "\n",
    "api = twitter.Api(consumer_key=params[\"consumer_key\"],\n",
    "                  consumer_secret=params[\"consumer_secret\"],\n",
    "                  access_token_key=params[\"access_token\"],\n",
    "                  access_token_secret=params[\"access_token_secret\"])\n",
    "\n",
    "\n",
    "with open(\"wsj.json\", \"wt\") as dj:\n",
    "    dj.write(\"[]\")\n",
    "with open(\"business.json\", \"wt\") as dj:\n",
    "    dj.write(\"[]\")\n",
    "with open(\"financialtimes.json\", \"wt\") as dj:\n",
    "    dj.write(\"[]\")\n",
    "with open(\"nytimes.json\", \"wt\") as dj:\n",
    "    dj.write(\"[]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first lines of code will initialize the API and allow you to make calls. The second part is what we use for our buffers to dump data from each news source. Note: you can get up to 200 tweets with a single call to the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will get tweets for each Twitter handle we are dealing with. Remember, there is a 200 tweet limit, so we need to spin the call to the API in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def del_last_char(filename):\n",
    "    with open(filename, 'rb+') as f:\n",
    "        f.seek(-1, os.SEEK_END)\n",
    "        f.truncate()\n",
    "\n",
    "def get_lots_of_tweets(handle, jsonfile, mxid=None):\n",
    "    statuses = api.GetUserTimeline(screen_name=handle, count=200, max_id=mxid)\n",
    "    del_last_char(jsonfile)\n",
    "    with open(jsonfile, \"a+\") as dj:\n",
    "        json.dump([json.loads(str(statuses[i])) for i in xrange(len(statuses))], dj)\n",
    "        dj.write(\"]\")\n",
    "    for _ in xrange(4):\n",
    "        mxid = statuses[-1].id - 1\n",
    "        statuses = api.GetUserTimeline(screen_name=handle, count=200, max_id=mxid)\n",
    "        del_last_char(jsonfile)\n",
    "        with open(jsonfile, \"a+\") as dj:\n",
    "            dj.write(\",\")\n",
    "            json.dump([json.loads(str(statuses[i])) for i in xrange(len(statuses))], dj)\n",
    "            dj.write(\"]\")\n",
    "\n",
    "get_lots_of_tweets(\"@WSJ\",             \"wsj.json\"           )\n",
    "get_lots_of_tweets(\"@business\",        \"business.json\"      )\n",
    "get_lots_of_tweets(\"@FinancialTimes\",  \"financialtimes.json\")\n",
    "get_lots_of_tweets(\"@nytimes\",         \"nytimes.json\"       )\n",
    "            \n",
    "# BEWARE: creates a json which is a list of lists of tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the last 800 tweets for all of the given handles, we can put them into the cache files that we prepared before when we were initializing our API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"wsj.json\", \"rt\") as dj:\n",
    "    wt = json.load(dj)\n",
    "with open(\"business.json\", \"rt\") as dj:\n",
    "    bt = json.load(dj)\n",
    "with open(\"financialtimes.json\", \"rt\") as dj:\n",
    "    ft = json.load(dj)\n",
    "with open(\"nytimes.json\", \"rt\") as dj:\n",
    "    nt = json.load(dj)\n",
    "    \n",
    "wsj_tweets = []\n",
    "for lst in wt:\n",
    "    wsj_tweets.extend(lst)\n",
    "business_tweets = []\n",
    "for lst in bt:\n",
    "    business_tweets.extend(lst)\n",
    "financialtimes_tweets = []\n",
    "for lst in ft:\n",
    "    financialtimes_tweets.extend(lst)\n",
    "nytimes_tweets = []\n",
    "for lst in nt:\n",
    "    nytimes_tweets.extend(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will put the tweets into dataframes, so that we can very easily perform data analysis on them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wsj = pd.DataFrame(wsj_tweets)\n",
    "wsj[\"created_at\"] = pd.to_datetime(wsj[\"created_at\"]) #convert to datetime for easier time series analysis\n",
    "business = pd.DataFrame(business_tweets)\n",
    "business[\"created_at\"] = pd.to_datetime(business[\"created_at\"])\n",
    "financialtimes = pd.DataFrame(financialtimes_tweets)\n",
    "financialtimes[\"created_at\"] = pd.to_datetime(financialtimes[\"created_at\"])\n",
    "nytimes = pd.DataFrame(nytimes_tweets)\n",
    "nytimes[\"created_at\"] = pd.to_datetime(nytimes[\"created_at\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a partial view of the wsj dataframe that was created (we will be doing more with this later).\n",
    "\n",
    "![Viz](https://s22.postimg.org/e0sxs3dfl/image.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Gathering of Data - Stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we need to gather the data, but this time for the stocks mentioned above. To get the data, you have to manually export it as follows: https://support.google.com/finance/answer/71913?hl=en. We did that, and obtained these three CSV's, and made dataframes for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "goog = pd.read_csv(\"goog.csv\")\n",
    "goog.columns = [\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "goog[\"Date\"] = pd.to_datetime(goog[\"Date\"])\n",
    "\n",
    "xom = pd.read_csv(\"xom.csv\")\n",
    "xom.columns = [\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "xom[\"Date\"] = pd.to_datetime(xom[\"Date\"])\n",
    "\n",
    "c = pd.read_csv(\"c.csv\")\n",
    "c.columns = [\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "c[\"Date\"] = pd.to_datetime(c[\"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print goog.head()\n",
    "print xom.head()\n",
    "print c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Viz](https://s12.postimg.org/q93xbhmvx/image.png \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "\n",
    "Running this call will show you that we have successfully created dataframes for each of the stocks that we will be analyzing in this report. We have curated relevant data, including timestamp, prices, and volume. Now, its time to do some visualizations with the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Visualization \n",
    "\n",
    "As mentioned earlier, we will be using matplotlib to make effective visualizations of the stock prices and hopefully be able to correlate dips and rises in price with relevant news items extracted above (the tweet data comes with dates, and this will be key in determining correlation). The following code will create a graph of historical data for each stock from around May to November 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(goog[\"Date\"], goog[\"Close\"])\n",
    "plt.plot(xom[\"Date\"], xom[\"Close\"])\n",
    "plt.plot(c[\"Date\"], c[\"Close\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Viz](https://s17.postimg.org/x2v6dn6gf/untitled.png \"Logo Title Text 1\")\n",
    "\n",
    "This shows a visualization each of the stocks. The next step would be to manually look into the Twitter data to see any critical news articles about the companies listed above (something like the Google Pixel release announcement, or a critical scientific discovery about oil discovery) can be classified as turning points for the stock.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
